{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5681ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "\n",
    "class PDFParser:\n",
    "    \"\"\"\n",
    "    ‚úÖ PDF Parser using Docling\n",
    "    Converts PDF into Docling Document object.\n",
    "    No Markdown export here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.converter = DocumentConverter()\n",
    "        self.doc = None\n",
    "\n",
    "    def parse_pdf(self, pdf_path: str):\n",
    "        \"\"\"\n",
    "        Parse PDF into Docling Document object.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to input PDF\n",
    "\n",
    "        Returns:\n",
    "            bool: True if parse successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = self.converter.convert(pdf_path)\n",
    "            self.doc = result.document\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to parse PDF: {e}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_cleaning(self):\n",
    "        \"\"\"\n",
    "        Optional: placeholder for any post-processing or cleaning.\n",
    "        \"\"\"\n",
    "        if self.doc is None:\n",
    "            print(\"‚ö†Ô∏è No document to analyze.\")\n",
    "            return\n",
    "        # Example: count pages\n",
    "        page_count = getattr(self.doc, 'page_count', 0)\n",
    "        print(f\"üìÑ Document has {page_count} pages.\")\n",
    "\n",
    "    def print_report(self):\n",
    "        \"\"\"Print basic document info\"\"\"\n",
    "        if self.doc is None:\n",
    "            print(\"‚ö†Ô∏è No document parsed yet.\")\n",
    "            return\n",
    "        print(\"üìä Docling Document Report\")\n",
    "        print(f\"   Title: {getattr(self.doc, 'title', 'Unknown')}\")\n",
    "        print(f\"   Pages: {getattr(self.doc, 'page_count', 'Unknown')}\")\n",
    "        print(f\"   Sections: {len(getattr(self.doc, 'sections', []))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4710dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from docling.chunking import HybridChunker\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGChunk:\n",
    "    content: str\n",
    "    metadata: dict\n",
    "    token_count: int\n",
    "\n",
    "\n",
    "class DoclingHybridChunker:\n",
    "    \"\"\"\n",
    "    ‚úÖ Hierarchical + Hybrid Chunker\n",
    "    Tables are chunked as a whole\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_tokens: int = 1024, overlap_tokens: int = 128):\n",
    "        embed_model = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embed_model)\n",
    "\n",
    "        self.hf_tokenizer = HuggingFaceTokenizer(\n",
    "            tokenizer=tokenizer,\n",
    "            max_tokens=max_tokens,\n",
    "            overlap_tokens=overlap_tokens\n",
    "        )\n",
    "\n",
    "        self.chunker = HybridChunker(\n",
    "            tokenizer=self.hf_tokenizer,\n",
    "            merge_peers=True,\n",
    "            descriptive_titles=True\n",
    "        )\n",
    "        self.chunks: List[RAGChunk] = []\n",
    "\n",
    "    def chunk_document(self, docling_doc) -> List[RAGChunk]:\n",
    "        chunk_iter = self.chunker.chunk(dl_doc=docling_doc)\n",
    "        chunk_id = 0\n",
    "\n",
    "        for chunk in chunk_iter:\n",
    "            content = chunk.text.strip() if hasattr(chunk, 'text') else \"\"\n",
    "            if len(content) < 50 and not getattr(chunk, 'tables', []):\n",
    "                continue\n",
    "\n",
    "            # Tables as whole chunks\n",
    "            if getattr(chunk, 'tables', []):\n",
    "                for table_idx, table in enumerate(chunk.tables):\n",
    "                    table_content = str(table)\n",
    "                    metadata = {\n",
    "                        'chunk_id': f'hybrid_{chunk_id:04d}_table_{table_idx}',\n",
    "                        'page_no': getattr(chunk, 'page_number', None) or getattr(chunk, 'page_no', 1),\n",
    "                        'hierarchy_level': getattr(chunk, 'hierarchy_level', 0),\n",
    "                        'doc_items_count': getattr(chunk, 'num_items', 1),\n",
    "                        'has_tables': True,\n",
    "                        'token_count_original': len(table_content.split()),\n",
    "                        'title': getattr(chunk, 'title', None)\n",
    "                    }\n",
    "                    token_count = len(self.hf_tokenizer.tokenizer.encode(table_content))\n",
    "                    self.chunks.append(RAGChunk(\n",
    "                        content=table_content,\n",
    "                        metadata=metadata,\n",
    "                        token_count=token_count\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "\n",
    "            # Text chunk\n",
    "            if content:\n",
    "                metadata = {\n",
    "                    'chunk_id': f'hybrid_{chunk_id:04d}',\n",
    "                    'page_no': getattr(chunk, 'page_number', None) or getattr(chunk, 'page_no', 1),\n",
    "                    'hierarchy_level': getattr(chunk, 'hierarchy_level', 0),\n",
    "                    'doc_items_count': getattr(chunk, 'num_items', 1),\n",
    "                    'has_tables': len(getattr(chunk, 'tables', [])) > 0,\n",
    "                    'token_count_original': getattr(chunk, 'token_count', len(content.split())),\n",
    "                    'title': getattr(chunk, 'title', None)\n",
    "                }\n",
    "                token_count = len(self.hf_tokenizer.tokenizer.encode(content))\n",
    "                self.chunks.append(RAGChunk(\n",
    "                    content=content,\n",
    "                    metadata=metadata,\n",
    "                    token_count=token_count\n",
    "                ))\n",
    "                chunk_id += 1\n",
    "\n",
    "        print(f\"‚úÖ HYBRID CHUNKER: {len(self.chunks)} semantic chunks created!\")\n",
    "        print(f\"   üìä Chunks per page: avg {len(self.chunks)/getattr(docling_doc, 'page_count', 1):.1f}\")\n",
    "        return self.chunks\n",
    "\n",
    "    def print_samples(self, n=3):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üè∑Ô∏è  DOC LING HYBRID CHUNKS (Hierarchical + Token-aware)\")\n",
    "        print(\"=\"*80)\n",
    "        for i, chunk in enumerate(self.chunks[:n]):\n",
    "            print(f\"\\n{i+1}. [{chunk.metadata['chunk_id']}] {chunk.token_count} tokens\")\n",
    "            print(f\"   üìÑ Page: {chunk.metadata['page_no']} | Level: {chunk.metadata['hierarchy_level']}\")\n",
    "            print(f\"   üìã Tables: {chunk.metadata['has_tables']} | Items: {chunk.metadata['doc_items_count']}\")\n",
    "            print(\"-\" * 70)\n",
    "            wrapped = textwrap.fill(chunk.content[:400], width=90)\n",
    "            print(wrapped)\n",
    "            print()\n",
    "\n",
    "    def save_rag_chunks(self, output_file: str):\n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for chunk in self.chunks:\n",
    "                f.write(json.dumps({\n",
    "                    'content': chunk.content,\n",
    "                    'metadata': chunk.metadata,\n",
    "                    'token_count': chunk.token_count\n",
    "                }, ensure_ascii=False) + '\\n')\n",
    "        print(f\"üíæ Saved {len(self.chunks)} RAG chunks ‚Üí {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7be9826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 21:25:35,903 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2026-01-08 21:25:35,922 - INFO - Going to convert document batch...\n",
      "2026-01-08 21:25:35,923 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2026-01-08 21:25:35,933 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-08 21:25:35,936 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2026-01-08 21:25:35,943 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-08 21:25:35,946 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2026-01-08 21:25:35,947 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2026-01-08 21:25:35,948 - INFO - easyocr cannot be used because it is not installed.\n",
      "2026-01-08 21:25:36,049 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,059 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,063 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,071 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,072 [RapidOCR] main.py:50: Using C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ PHASE 1: Docling PDF Parsing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-01-08 21:25:36,182 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,183 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,185 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,185 [RapidOCR] main.py:50: Using C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,231 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,232 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,247 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2026-01-08 21:25:36,248 [RapidOCR] main.py:50: Using C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2026-01-08 21:25:36,386 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2026-01-08 21:25:36,394 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-08 21:25:36,397 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2026-01-08 21:25:36,402 - INFO - Accelerator device: 'cpu'\n",
      "2026-01-08 21:25:37,112 - INFO - Loading plugin 'docling_defaults'\n",
      "2026-01-08 21:25:37,114 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2026-01-08 21:25:37,510 - INFO - Accelerator device: 'cpu'\n",
      "2026-01-08 21:25:37,733 - INFO - Processing document R0r4e.pdf\n",
      "2026-01-08 21:26:13,421 - INFO - Finished converting document R0r4e.pdf in 37.52 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Document has 0 pages.\n",
      "üìä Docling Document Report\n",
      "   Title: Unknown\n",
      "   Pages: Unknown\n",
      "   Sections: 0\n",
      "\n",
      "üî™ PHASE 2: Hybrid Hierarchical Chunking...\n",
      "‚úÖ HYBRID CHUNKER: 42 semantic chunks created!\n",
      "   üìä Chunks per page: avg 42.0\n",
      "\n",
      "================================================================================\n",
      "üè∑Ô∏è  DOC LING HYBRID CHUNKS (Hierarchical + Token-aware)\n",
      "================================================================================\n",
      "\n",
      "1. [hybrid_0000] 76 tokens\n",
      "   üìÑ Page: 1 | Level: 0\n",
      "   üìã Tables: False | Items: 1\n",
      "----------------------------------------------------------------------\n",
      "Concerning the Adoption of Harmonized Technical United Nations Regulations for Wheeled\n",
      "Vehicles, Equipment and Parts which can be Fitted and/or be Used on Wheeled Vehicles and\n",
      "the Conditions for Reciprocal Recognition of Approvals Granted on the Basis of these\n",
      "United Nations Regulations * (Revision 3, including the amendments which entered into\n",
      "force on 14 September 2017) _________\n",
      "\n",
      "\n",
      "2. [hybrid_0001] 49 tokens\n",
      "   üìÑ Page: 1 | Level: 0\n",
      "   üìã Tables: False | Items: 1\n",
      "----------------------------------------------------------------------\n",
      "Incorporating all valid text up to: Revision 3 - Amendment 1 - Date of entry into force:\n",
      "22 June 2022 04 series of amendments - Date of entry into force: 22 June 2022\n",
      "\n",
      "\n",
      "3. [hybrid_0002] 34 tokens\n",
      "   üìÑ Page: 1 | Level: 0\n",
      "   üìã Tables: False | Items: 1\n",
      "----------------------------------------------------------------------\n",
      "This document is meant purely as documentation tool. The authentic and legal binding text\n",
      "is: ECE/TRANS/WP.29/2021/84. _________\n",
      "\n",
      "üíæ Saved 42 RAG chunks ‚Üí ./chunks/R0r4e_hybrid_tables.jsonl\n",
      "\n",
      "üéâ PIPELINE COMPLETE! 42 HIGH-QUALITY CHUNKS READY!\n",
      "   ‚úÖ Ready for LanceDB embedding ‚Üí RAG queries\n"
     ]
    }
   ],
   "source": [
    "# from pdf_parser import PDFParser\n",
    "# from hybrid_chunker import DoclingHybridChunker\n",
    "\n",
    "def run_pipeline(pdf_path: str, rag_output_path: str):\n",
    "    \"\"\"\n",
    "    Full PDF ‚Üí Docling ‚Üí Hybrid Chunking ‚Üí RAG JSONL pipeline\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to input PDF\n",
    "        rag_output_path (str): Path to save RAG-ready JSONL\n",
    "    \"\"\"\n",
    "    print(\"üîÑ PHASE 1: Docling PDF Parsing...\")\n",
    "    parser = PDFParser()\n",
    "    if not parser.parse_pdf(pdf_path):\n",
    "        print(\"‚ùå Parsing failed. Exiting pipeline.\")\n",
    "        return\n",
    "\n",
    "    parser.analyze_cleaning()\n",
    "    parser.print_report()\n",
    "\n",
    "    print(\"\\nüî™ PHASE 2: Hybrid Hierarchical Chunking...\")\n",
    "    chunker = DoclingHybridChunker(max_tokens=1024, overlap_tokens=128)\n",
    "    chunks = chunker.chunk_document(parser.doc)\n",
    "\n",
    "    chunker.print_samples(n=3)\n",
    "    chunker.save_rag_chunks(rag_output_path)\n",
    "\n",
    "    print(f\"\\nüéâ PIPELINE COMPLETE! {len(chunks)} HIGH-QUALITY CHUNKS READY!\")\n",
    "    print(\"   ‚úÖ Ready for LanceDB embedding ‚Üí RAG queries\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\data\\R0r4e.pdf\"\n",
    "    rag_output_path = r\"./chunks/R0r4e_hybrid_tables.jsonl\"\n",
    "\n",
    "    run_pipeline(pdf_path, rag_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7d9f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-08 21:30:54,460 - INFO - Use pytorch device_name: cpu\n",
      "2026-01-08 21:30:54,461 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created LanceDB table 'rag_hybrid_chunks' with mode='overwrite'\n",
      "   Vector dim: 768\n",
      "‚úÖ Loaded 42 chunks from JSONL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedded 42 chunks\n",
      "‚úÖ Stored 42 chunks in LanceDB\n",
      "\n",
      "üéâ Successfully stored 42 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç QUERY: Give me the regulation scope\n",
      "================================================================================\n",
      "üÜî chunk_000004 | Score: 0.418\n",
      "üìÑ Page 1 | 602 tokens\n",
      "üìù Regulation,  = . Regulation, Page = . 1.,  = Scope.................................................................................................................\n",
      "--------------------------------------------------------------------------------\n",
      "üÜî chunk_000002 | Score: 0.275\n",
      "üìÑ Page 1 | 34 tokens\n",
      "üìù This document is meant purely as documentation tool. The authentic and legal binding text is: ECE/TRANS/WP.29/2021/84.\n",
      "_________...\n",
      "--------------------------------------------------------------------------------\n",
      "üÜî chunk_000033 | Score: 0.267\n",
      "üìÑ Page 1 | 243 tokens\n",
      "üìù Supply the information required by the following table in respect of the applicable subjects for this vehicle in Annex 4. All relevant approvals for each subjec...\n",
      "--------------------------------------------------------------------------------\n",
      "üÜî chunk_000005 | Score: 0.238\n",
      "üìÑ Page 1 | 32 tokens\n",
      "üìù - 1.1. This Regulation applies to vehicles of category M1. 1  It specifies requirements for the type approval of a whole vehicle....\n",
      "--------------------------------------------------------------------------------\n",
      "üÜî chunk_000022 | Score: 0.230\n",
      "üìÑ Page 1 | 348 tokens\n",
      "üìù 1. Objectives and scope\n",
      "This  annex  sets  out  the  procedures  for  IWVTA  in  accordance  with  the provisions of paragraph 4.1. of this Regulation\n",
      "2. Type a...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä LANCE DB STATS\n",
      "   Total chunks: 42\n",
      "   Avg tokens: 385\n",
      "   Chunks with tables: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LanceDB RAG Store\n",
    "Embedding ‚Üí LanceDB ingestion ‚Üí Vector search\n",
    "\n",
    "- LanceDB >= 0.5\n",
    "- SentenceTransformers embeddings\n",
    "- JSONL hybrid chunks input (table-aware)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data model\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class LanceDBChunk:\n",
    "    id: str\n",
    "    vector: List[float]\n",
    "    content: str\n",
    "    metadata: str\n",
    "    token_count: int\n",
    "    page_no: int | None\n",
    "    has_tables: bool\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# LanceDB RAG Store\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "class LanceDBRAGStore:\n",
    "    \"\"\"\n",
    "    Hybrid RAG Vector Store using LanceDB\n",
    "\n",
    "    - Embedding: SentenceTransformers\n",
    "    - Storage: LanceDB\n",
    "    - Input: JSONL chunks (from DoclingHybridChunker)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str = \"./lancedb_rag\",\n",
    "        table_name: str = \"rag_hybrid_chunks\",\n",
    "        embed_model: str = \"BAAI/bge-base-en-v1.5\",\n",
    "    ):\n",
    "        self.db_path = Path(db_path)\n",
    "        self.db_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.embedder = SentenceTransformer(embed_model)\n",
    "        self.embedding_dim = self.embedder.get_sentence_embedding_dimension()\n",
    "\n",
    "        self.db = lancedb.connect(str(self.db_path))\n",
    "        self.table_name = table_name\n",
    "        self.table = None\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Table creation\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def create_table(self, overwrite: bool = True):\n",
    "        if overwrite and self.table_name in self.db.list_tables():\n",
    "            self.db.drop_table(self.table_name)\n",
    "\n",
    "        schema = pa.schema([\n",
    "            (\"id\", pa.string()),\n",
    "            (\"vector\", pa.list_(pa.float32(), self.embedding_dim)),\n",
    "            (\"content\", pa.string()),\n",
    "            (\"metadata\", pa.string()),\n",
    "            (\"token_count\", pa.int32()),\n",
    "            (\"page_no\", pa.int32()),\n",
    "            (\"has_tables\", pa.bool_()),\n",
    "        ])\n",
    "\n",
    "        mode = \"overwrite\" if overwrite else \"create\"\n",
    "\n",
    "        self.table = self.db.create_table(\n",
    "            self.table_name,\n",
    "            schema=schema,\n",
    "            mode=mode,  # <-- updated\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Created LanceDB table '{self.table_name}' with mode='{mode}'\")\n",
    "        print(f\"   Vector dim: {self.embedding_dim}\")\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Load chunks\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def load_chunks_from_jsonl(self, jsonl_path: str) -> List[LanceDBChunk]:\n",
    "        chunks: List[LanceDBChunk] = []\n",
    "\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                record = json.loads(line)\n",
    "                metadata = record.get(\"metadata\", {})\n",
    "\n",
    "                chunks.append(\n",
    "                    LanceDBChunk(\n",
    "                        id=f\"chunk_{idx:06d}\",\n",
    "                        vector=[],\n",
    "                        content=record[\"content\"],\n",
    "                        metadata=json.dumps(metadata),\n",
    "                        token_count=record.get(\"token_count\", 0),\n",
    "                        page_no=metadata.get(\"page_no\", 0),\n",
    "                        has_tables=metadata.get(\"has_tables\", False),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        print(f\"‚úÖ Loaded {len(chunks)} chunks from JSONL\")\n",
    "        return chunks\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Embedding\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def embed_chunks(self, chunks: List[LanceDBChunk]) -> List[LanceDBChunk]:\n",
    "        texts = [c.content for c in chunks]\n",
    "\n",
    "        embeddings = self.embedder.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True,\n",
    "        )\n",
    "\n",
    "        for chunk, vector in zip(chunks, embeddings):\n",
    "            chunk.vector = vector.tolist()\n",
    "\n",
    "        print(f\"‚úÖ Embedded {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Storage\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def store_chunks(self, chunks: List[LanceDBChunk]):\n",
    "        df = pd.DataFrame([{\n",
    "            \"id\": c.id,\n",
    "            \"vector\": c.vector,\n",
    "            \"content\": c.content,\n",
    "            \"metadata\": c.metadata,\n",
    "            \"token_count\": c.token_count,\n",
    "            \"page_no\": c.page_no or 0,\n",
    "            \"has_tables\": c.has_tables,\n",
    "        } for c in chunks])\n",
    "\n",
    "        self.table.add(df)\n",
    "        print(f\"‚úÖ Stored {len(chunks)} chunks in LanceDB\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # End-to-end ingestion\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def ingest(self, jsonl_path: str):\n",
    "        self.create_table(overwrite=True)\n",
    "        chunks = self.load_chunks_from_jsonl(jsonl_path)\n",
    "        chunks = self.embed_chunks(chunks)\n",
    "        self.store_chunks(chunks)\n",
    "\n",
    "        return len(chunks)\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Query\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def query(self, query_text: str, k: int = 5):\n",
    "        query_vec = self.embedder.encode(\n",
    "            query_text,\n",
    "            normalize_embeddings=True,\n",
    "        )\n",
    "\n",
    "        results = (\n",
    "            self.table.search(query_vec)\n",
    "            .limit(k)\n",
    "            .to_pandas()\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüîç QUERY: {query_text}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for _, row in results.iterrows():\n",
    "            score = 1 - row[\"_distance\"]\n",
    "            print(f\"üÜî {row['id']} | Score: {score:.3f}\")\n",
    "            print(f\"üìÑ Page {row['page_no']} | {row['token_count']} tokens\")\n",
    "            print(f\"üìù {row['content'][:160]}...\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "        return results\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Stats\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def stats(self):\n",
    "        df = self.table.to_pandas()\n",
    "\n",
    "        print(\"\\nüìä LANCE DB STATS\")\n",
    "        print(f\"   Total chunks: {len(df)}\")\n",
    "        print(f\"   Avg tokens: {df['token_count'].mean():.0f}\")\n",
    "        print(f\"   Chunks with tables: {df['has_tables'].sum()}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    store = LanceDBRAGStore()\n",
    "\n",
    "    chunks_file = \"./chunks/R0r4e_hybrid_tables.jsonl\"  # Updated for table-aware chunks\n",
    "    count = store.ingest(chunks_file)\n",
    "\n",
    "    print(f\"\\nüéâ Successfully stored {count} chunks\")\n",
    "\n",
    "    store.query(\"Give me the regulation scope\")\n",
    "    store.stats()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820dcdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\birok\\Python\\LLMOPs\\docling-tutorials\\docling-venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "2026-01-08 21:33:05,874 - INFO - Use pytorch device_name: cpu\n",
      "2026-01-08 21:33:05,876 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to LanceDB table 'rag_hybrid_chunks'\n",
      "‚úÖ Embedder: BAAI/bge-base-en-v1.5\n",
      "‚úÖ ChatGroq: llama-3.3-70b-versatile\n",
      "\n",
      "üéâ RAG CHAT READY!\n",
      "\n",
      "üîç Q: What is the scope of this regulation?\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 16.52it/s]\n",
      "2026-01-08 21:33:11,297 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ A: The scope of this regulation is found on Page 1 and further detailed on Page 4 of SOURCE 1. According to SOURCE 1 (Page 1), the regulation outlines its structure, including the scope, which is elaborated on Page 4. Additionally, SOURCE 2 (Page 1) specifies that this regulation applies to vehicles of category M1 and outlines requirements for the type approval of a whole vehicle.\n",
      "\n",
      "üìÑ SOURCES (4 found):\n",
      "  1. Page 1 (Score: 0.359)\n",
      "  2. Page 1 (Score: 0.311)\n",
      "  3. Page 1 (Score: 0.264)\n",
      "  4. Page 1 (Score: 0.248)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üî• Interactive chat (type 'quit' to exit):\n",
      "üëã Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "‚úÖ FIXED LanceDB ‚Üí ChatGroq RAG (Connect & QA Only)\n",
    "WORKS WITH YOUR EXISTING DATABASE!\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import lancedb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class LanceDBChatGroq:\n",
    "    \"\"\"Connect to existing LanceDB ‚Üí ChatGroq RAG\"\"\"\n",
    "\n",
    "    def __init__(self, db_path: str = \"./lancedb_rag\", table_name: str = \"rag_hybrid_chunks\"):\n",
    "        self.db_path = Path(db_path)\n",
    "        self.table_name = table_name\n",
    "\n",
    "        # Load existing LanceDB table\n",
    "        self.db = lancedb.connect(str(self.db_path))\n",
    "        self.table = self.db.open_table(self.table_name)\n",
    "\n",
    "        # ‚úÖ Match the embedding model used during ingestion\n",
    "        self.embedder = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "        # ‚úÖ ChatGroq LLM\n",
    "        self.llm = ChatGroq(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Connected to LanceDB table '{self.table_name}'\")\n",
    "        print(f\"‚úÖ Embedder: BAAI/bge-base-en-v1.5\")\n",
    "        print(f\"‚úÖ ChatGroq: llama-3.3-70b-versatile\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Retrieval\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4):\n",
    "        \"\"\"Vector search using stored embeddings\"\"\"\n",
    "        query_emb = self.embedder.encode([query])[0]\n",
    "        results = self.table.search(query_emb).limit(k).to_pandas()\n",
    "\n",
    "        context_docs = []\n",
    "        for _, row in results.iterrows():\n",
    "            doc = {\n",
    "                \"page_no\": row[\"page_no\"],\n",
    "                \"content\": row[\"content\"],\n",
    "                \"score\": 1 - row[\"_distance\"]\n",
    "            }\n",
    "            context_docs.append(doc)\n",
    "\n",
    "        return context_docs\n",
    "\n",
    "    def format_context(self, docs):\n",
    "        \"\"\"Format retrieved chunks for the LLM\"\"\"\n",
    "        context = \"\"\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            context += f\"\\n\\n--- SOURCE {i} (Page {doc['page_no']}, Score: {doc['score']:.3f}) ---\\n\"\n",
    "            context += doc[\"content\"]\n",
    "        return context\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Response generation\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def generate_response(self, question: str, context: str):\n",
    "        \"\"\"Generate answer with ChatGroq\"\"\"\n",
    "        prompt_template = \"\"\"\n",
    "You are a helpful assistant specialized in regulation documents.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Use ONLY the provided context below\n",
    "2. Cite specific page numbers when possible\n",
    "3. If answer not in context, say \"Not found in document\"\n",
    "4. Be precise and professional\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER (include page citations):\n",
    "\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "        chain = prompt | self.llm\n",
    "\n",
    "        response = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "\n",
    "        return response.content\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Full RAG query\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    def query(self, question: str, k: int = 4):\n",
    "        \"\"\"Complete RAG pipeline: retrieve ‚Üí LLM answer\"\"\"\n",
    "        print(f\"\\nüîç Q: {question}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Retrieve relevant chunks\n",
    "        docs = self.retrieve(question, k=k)\n",
    "        context = self.format_context(docs)\n",
    "\n",
    "        # Generate answer\n",
    "        answer = self.generate_response(question, context)\n",
    "\n",
    "        print(f\"ü§ñ A: {answer}\")\n",
    "        print(f\"\\nüìÑ SOURCES ({len(docs)} found):\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"  {i}. Page {doc['page_no']} (Score: {doc['score']:.3f})\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        return answer, docs\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# RUN INTERACTIVE CHAT\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_chat = LanceDBChatGroq(\"./lancedb_rag\")\n",
    "\n",
    "    print(\"\\nüéâ RAG CHAT READY!\")\n",
    "\n",
    "    # Optional: test query\n",
    "    rag_chat.query(\"What is the scope of this regulation?\")\n",
    "\n",
    "    print(\"\\nüî• Interactive chat (type 'quit' to exit):\")\n",
    "    while True:\n",
    "        question = input(\"\\n‚ùì Ask: \").strip()\n",
    "        if question.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"üëã Exiting chat.\")\n",
    "            break\n",
    "        rag_chat.query(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af433a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docling-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
